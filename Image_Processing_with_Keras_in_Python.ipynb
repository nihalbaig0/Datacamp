{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Processing with Keras in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO+yHgh5lYZnKfLFzYMHqaM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihalbaig0/Datacamp/blob/master/Image_Processing_with_Keras_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjYJNXNF7640",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93YBcuEC9A7M",
        "colab_type": "text"
      },
      "source": [
        "Images as data: visualizations\n",
        "To display image data, you will rely on Python's Matplotlib library, and specifically use matplotlib's pyplot sub-module, that contains many plotting commands. Some of these commands allow you to display the content of images stored in arrays.\n",
        "`Import the image from the file bricks.png into data.\n",
        "Display the image in data on the screen.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR1jw5MY9MEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "4020c248-31c6-40a3-c691-a52d8ca5a73e"
      },
      "source": [
        "# Import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image\n",
        "data = plt.imread('bricks.png')\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f99d5e7f19be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bricks.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Display the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1472\u001b[0m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bricks.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KyrRwJ6-2LS",
        "colab_type": "text"
      },
      "source": [
        "Images as data: changing images\n",
        "To modify an image, you can modify the existing numbers in the array. In a color image, you can change the values in one of the color channels without affecting the other colors, by indexing on the last dimension of the array.\n",
        "\n",
        "The image you imported in the previous exercise is available in data.\n",
        "`Modify the bricks image to replace the top left corner of the image (10 by 10 pixels) into a red square.\n",
        "Visualize the resulting image.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccbDB9ny-7rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the red channel in this part of the image to 1\n",
        "data[:10,:10,0] = 1\n",
        "\n",
        "# Set the green channel in this part of the image to 0\n",
        "data[:10,:10,1] = 0\n",
        "\n",
        "# Set the blue channel in this part of the image to 0\n",
        "data[:10,:10,2] = 0\n",
        "\n",
        "# Visualize the result\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp_su98b--OD",
        "colab_type": "text"
      },
      "source": [
        "Using one-hot encoding to represent images\n",
        "Neural networks expect the labels of classes in a dataset to be organized in a one-hot encoded manner: each row in the array contains zeros in all columns, except the column corresponding to a unique label, which is set to 1.\n",
        "\n",
        "The fashion dataset contains three categories:\n",
        "\n",
        "Shirts\n",
        "Dresses\n",
        "Shoes\n",
        "In this exercise, you will create a one-hot encoding of a small sample of these labels.\n",
        "`Initialize the ohe_labels variable to hold the one-hot encoded array.\n",
        "Use np.where() to find the location of the category of the item in each iteration in categories.\n",
        "Assign a 1 into the correct row/column combination in every iteration.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXuvS8JfBGnx",
        "colab_type": "text"
      },
      "source": [
        "Evaluating a classifier\n",
        "To evaluate a classifier, we need to test it on images that were not used during training. This is called \"cross-validation\": a prediction of the class (e.g., t-shirt, dress or shoe) is made from each of the test images, and these predictions are compared with the true labels of these images.\n",
        "\n",
        "The results of cross-validation are provided as one-hot encoded arrays: test_labels and predictions.\n",
        "`Multiply the arrays with each other and sum the result to find the total number of correct predictions.\n",
        "Divide the number of correct answers (the sum) by the length of predictions array to calculate the proportion of correct predictions.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHNYFp-pBMEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the number of correct predictions\n",
        "number_correct = (test_labels * predictions).sum()\n",
        "print(number_correct)\n",
        "\n",
        "# Calculate the proportion of correct predictions\n",
        "proportion_correct = number_correct / len(predictions)\n",
        "print(proportion_correct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EOAdDeiBPEB",
        "colab_type": "text"
      },
      "source": [
        "Build a neural network\n",
        "We will use the Keras library to create neural networks and to train these neural networks to classify images. These models will all be of the Sequential type, meaning that the outputs of one layer are provided as inputs only to the next layer.\n",
        "\n",
        "In this exercise, you will create a neural network with Dense layers, meaning that each unit in each layer is connected to all of the units in the previous layer. For example, each unit in the first layer is connected to all of the pixels in the input images. The Dense layer object receives as arguments the number of units in that layer, and the activation function for the units. For the first layer in the network, it also receives an input_shape keyword argument.\n",
        "\n",
        "This course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the Keras Cheat Sheet and keep it handy!\n",
        "`The first layer receives images as input, has 10 units and 'relu' activation.\n",
        "The second input layer has 10 units and 'relu' activation.\n",
        "The output layer has one unit for each category (3 categories) and 'softmax' activation.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVIbxnFECqX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports components from Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initializes a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# First layer\n",
        "model.add(Dense(10, activation='relu', input_shape=(784,)))\n",
        "\n",
        "# Second layer\n",
        "model.add(Dense(10, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(3,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb31w0VyCyTt",
        "colab_type": "text"
      },
      "source": [
        "Compile a neural network\n",
        "Once you have constructed a model in Keras, the model needs to be compiled before you can fit it to data. This means that you need to specify the optimizer that will be used to fit the model and the loss function that will be used in optimization. Optionally, you can also specify a list of metrics that the model will keep track of. For example, if you want to know the classification accuracy, you will provide the list ['accuracy'] to the metrics keyword argument.\n",
        "`Write code to compile the model with the 'adam' optimizer and 'categorical_crossentropy' as the loss function.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-g8_J8zDALg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', \n",
        "           loss='categorical_crossentropy', \n",
        "           metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whR2DtVcDCSt",
        "colab_type": "text"
      },
      "source": [
        "Fitting a neural network model to clothing data\n",
        "In this exercise, you will fit the fully connected neural network that you constructed in the previous exercise to image data. The training data is provided as two variables: train_data that contains the pixel data for 50 images of the three clothing classes and train_labels, which contains one-hot encoded representations of the labels for each one of these 50 images. Transform the data into the network's expected input and then fit the model on training data and training labels.\n",
        "\n",
        "The model you compiled in the previous exercise, and train_data and train_labels are available in your workspace.\n",
        "`Prepare the data for fitting by reshaping it.\n",
        "Fit the model by passing the input training data and training labels to the model's .fit() method.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1YYDv93DUKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the data to two-dimensional array\n",
        "train_data = train_data.reshape(50, 28*28)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(train_data, train_labels, validation_split=0.2, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E58WDX4sDWVr",
        "colab_type": "text"
      },
      "source": [
        "Cross-validation for neural network evaluation\n",
        "To evaluate the model, we use a separate test data-set. As in the train data, the images in the test data also need to be reshaped before they can be provided to the fully-connected network because the network expects one column per pixel in the input.\n",
        "\n",
        "The model you fit in the previous exercise, and test_data and test_labels are available in your workspace.\n",
        "`Reshape the test_data so that it can be used to evaluate the model.\n",
        "Evaluate the model on test_data using test_labels.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yRD61ojDwD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape test data\n",
        "test_data = test_data.reshape(10,28*28)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(test_data, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUSYcdfiDzNh",
        "colab_type": "text"
      },
      "source": [
        "One dimensional convolutions\n",
        "A convolution of an one-dimensional array with a kernel comprises of taking the kernel, sliding it along the array, multiplying it with the items in the array that overlap with the kernel in that location and summing this product.\n",
        "`Multiply each window in the input array with the kernel and sum the multiplied result and allocate the result into the correct entry in the output array (conv).`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qJDa2EjF8K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "array = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
        "kernel = np.array([1, -1, 0])\n",
        "conv = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "# Output array\n",
        "for ii in range(8):\n",
        "    conv[ii] = (kernel * array[ii:ii+3]).sum()\n",
        "\n",
        "# Print conv\n",
        "print(conv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FO8UUuaF-RD",
        "colab_type": "text"
      },
      "source": [
        "Image convolutions\n",
        "The convolution of an image with a kernel summarizes a part of the image as the sum of the multiplication of that part of the image with the kernel. In this exercise, you will write the code that executes a convolution of an image with a kernel using Numpy. Given a black and white image that is stored in the variable im, write the operations inside the loop that would execute the convolution with the provided kernel.\n",
        "`Select the right window from the image in each iteration and multiply this part of the image with the kernel.\n",
        "Sum the result and allocate the sum to the correct entry in the output array (results).\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzp4rOe9HqDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
        "result = np.zeros(im.shape)\n",
        "\n",
        "# Output array\n",
        "for ii in range(im.shape[0] - 3):\n",
        "    for jj in range(im.shape[1] - 3):\n",
        "        result[ii, jj] = (im[ii:ii+3, jj:jj+3] * kernel).sum()\n",
        "\n",
        "# Print result\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSiy3YfKHsJ7",
        "colab_type": "text"
      },
      "source": [
        "Defining image convolution kernels\n",
        "In the previous exercise, you wrote code that performs a convolution given an image and a kernel. This code is now stored in a function called convolution() that takes two inputs: image and kernel and produces the convolved image. In this exercise, you will be asked to define the kernel that finds a particular feature in the image.\n",
        "\n",
        "For example, the following kernel finds a vertical line in images:\n",
        "\n",
        "np.array([[-1, 1, -1], \n",
        "          [-1, 1, -1], \n",
        "          [-1, 1, -1]])\n",
        "          `Define a kernel that finds a dark spot surrounded by bright pixels.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnz3BqFeJUQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel = np.array([[1, 1, 1], \n",
        "                   [1, -1, 1],\n",
        "                   [1, 1, 1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41tnRbT5JWYW",
        "colab_type": "text"
      },
      "source": [
        "Convolutional network for image classification\n",
        "Convolutional networks for classification are constructed from a sequence of convolutional layers (for image processing) and fully connected (Dense) layers (for readout). In this exercise, you will construct a small convolutional network for classification of the data from the fashion dataset.\n",
        "`Add a Conv2D layer to construct the input layer of the network. Use a kernel size of 3 by 3. You can use the img_rows and img_cols objects available in your workspace to define the input_shape of this layer.\n",
        "Add a Flatten layer to translate between the image processing and classification part of your network.\n",
        "Add a Dense layer to classify the 3 different categories of clothing in the dataset.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC5KGyr4K4OT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary components from Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "# Initialize the model object\n",
        "model = Sequential()\n",
        "\n",
        "# Add a convolutional layer\n",
        "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
        "               input_shape=(img_rows,img_cols,1)))\n",
        "\n",
        "# Flatten the output of the convolutional layer\n",
        "model.add(Flatten())\n",
        "# Add an output layer for the 3 categories\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P68gBGVlK6KU",
        "colab_type": "text"
      },
      "source": [
        "Training a CNN to classify clothing types\n",
        "Before training a neural network it needs to be compiled with the right cost function, using the right optimizer. During compilation, you can also define metrics that the network calculates and reports in every epoch. Model fitting requires a training data set, together with the training labels to the network.\n",
        "\n",
        "The Conv2D model you built in the previous exercise is available in your workspace.\n",
        "`Compile the network using the 'adam' optimizer and the 'categorical_crossentropy' cost function. In the metrics list define that the network to report 'accuracy'.\n",
        "Fit the network on train_data and train_labels. Train for 3 epochs with a batch size of 10 images. In training, set aside 20% of the data as a validation set, using the validation_split keyword argument.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_QhjR6oLLbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model \n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on a training set\n",
        "model.fit(train_data, train_labels, \n",
        "          validation_split=0.2, \n",
        "          epochs=3, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlhel45WLNzz",
        "colab_type": "text"
      },
      "source": [
        "To evaluate a trained neural network, you should provide a separate testing data set of labeled images. The model you fit in the previous exercise is available in your workspace.\n",
        "`Evaluate the data on a separate test set: test_data and test_labels.\n",
        "Use the same batch size that was used for fitting (10 images per batch).`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHNCryYdLacp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the model on separate test data\n",
        "model.evaluate(test_data,test_labels,batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgsieXAXLck8",
        "colab_type": "text"
      },
      "source": [
        "Add padding to a CNN\n",
        "Padding allows a convolutional layer to retain the resolution of the input into this layer. This is done by adding zeros around the edges of the input image, so that the convolution kernel can overlap with the pixels on the edge of the image.\n",
        "`Add a Conv2D layer and choose a padding such that the output has the same size as the input.\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vqjErZ5Mvgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the convolutional layer\n",
        "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
        "                 input_shape=(img_rows, img_cols, 1), \n",
        "                 padding='same'))\n",
        "\n",
        "# Feed into output layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04gfStcSMxVr",
        "colab_type": "text"
      },
      "source": [
        "Add strides to a convolutional network\n",
        "The size of the strides of the convolution kernel determines whether the kernel will skip over some of the pixels as it slides along the image. This affects the size of the output because when strides are larger than one, the kernel will be centered on only some of the pixels.\n",
        "`Construct a neural network with a Conv2D layer with strided convolutions that skips every other pixel.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2FaeA6LM6wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the convolutional layer\n",
        "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
        "              input_shape=(img_rows, img_cols, 1), \n",
        "              strides=2))\n",
        "\n",
        "# Feed into output layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_2omCUjM83D",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}